{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\"Thank you very much, everybody. And congratulations to the class of 2017. That’s some achievement.This is your day and you’ve earned every minute of it. And I’m thrilled to be back at Liberty University. I’ve been here, this is now my third time. And we love setting records, right? We always set records. We have to set records, we have no choice.It’s been a little over a year since I’ve spoken on your beautiful campus and so much has changed. Right here, the class of 2017, dressed in cap and gown, graduating to a totally brilliant future. And here I am standing before you as president of the United States. So I’m guessing there are some people here today who thought that either one of those things, either one, would really require major help from God. Do we agree? And we got it.But here we are celebrating together on this very joyous occasion. And there is no place in the world I’d rather be to give my first commencement address as president than here with my wonderful friends at Liberty University.And I accepted this invitation a long time ago. I said to Jerry that I’d be there, and when I say something I mean it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)  # this is to convert paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Thank you very much, everybody.',\n",
       " 'And congratulations to the class of 2017.',\n",
       " 'That’s some achievement.This is your day and you’ve earned every minute of it.',\n",
       " 'And I’m thrilled to be back at Liberty University.',\n",
       " 'I’ve been here, this is now my third time.',\n",
       " 'And we love setting records, right?',\n",
       " 'We always set records.',\n",
       " 'We have to set records, we have no choice.It’s been a little over a year since I’ve spoken on your beautiful campus and so much has changed.',\n",
       " 'Right here, the class of 2017, dressed in cap and gown, graduating to a totally brilliant future.',\n",
       " 'And here I am standing before you as president of the United States.',\n",
       " 'So I’m guessing there are some people here today who thought that either one of those things, either one, would really require major help from God.',\n",
       " 'Do we agree?',\n",
       " 'And we got it.But here we are celebrating together on this very joyous occasion.',\n",
       " 'And there is no place in the world I’d rather be to give my first commencement address as president than here with my wonderful friends at Liberty University.And I accepted this invitation a long time ago.',\n",
       " 'I said to Jerry that I’d be there, and when I say something I mean it.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we have to convert sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'very',\n",
       " 'much',\n",
       " ',',\n",
       " 'everybody',\n",
       " '.',\n",
       " 'And',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'class',\n",
       " 'of',\n",
       " '2017',\n",
       " '.',\n",
       " 'That',\n",
       " '’',\n",
       " 's',\n",
       " 'some',\n",
       " 'achievement.This',\n",
       " 'is',\n",
       " 'your',\n",
       " 'day',\n",
       " 'and',\n",
       " 'you',\n",
       " '’',\n",
       " 've',\n",
       " 'earned',\n",
       " 'every',\n",
       " 'minute',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " 'And',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'thrilled',\n",
       " 'to',\n",
       " 'be',\n",
       " 'back',\n",
       " 'at',\n",
       " 'Liberty',\n",
       " 'University',\n",
       " '.',\n",
       " 'I',\n",
       " '’',\n",
       " 've',\n",
       " 'been',\n",
       " 'here',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'now',\n",
       " 'my',\n",
       " 'third',\n",
       " 'time',\n",
       " '.',\n",
       " 'And',\n",
       " 'we',\n",
       " 'love',\n",
       " 'setting',\n",
       " 'records',\n",
       " ',',\n",
       " 'right',\n",
       " '?',\n",
       " 'We',\n",
       " 'always',\n",
       " 'set',\n",
       " 'records',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'to',\n",
       " 'set',\n",
       " 'records',\n",
       " ',',\n",
       " 'we',\n",
       " 'have',\n",
       " 'no',\n",
       " 'choice.It',\n",
       " '’',\n",
       " 's',\n",
       " 'been',\n",
       " 'a',\n",
       " 'little',\n",
       " 'over',\n",
       " 'a',\n",
       " 'year',\n",
       " 'since',\n",
       " 'I',\n",
       " '’',\n",
       " 've',\n",
       " 'spoken',\n",
       " 'on',\n",
       " 'your',\n",
       " 'beautiful',\n",
       " 'campus',\n",
       " 'and',\n",
       " 'so',\n",
       " 'much',\n",
       " 'has',\n",
       " 'changed',\n",
       " '.',\n",
       " 'Right',\n",
       " 'here',\n",
       " ',',\n",
       " 'the',\n",
       " 'class',\n",
       " 'of',\n",
       " '2017',\n",
       " ',',\n",
       " 'dressed',\n",
       " 'in',\n",
       " 'cap',\n",
       " 'and',\n",
       " 'gown',\n",
       " ',',\n",
       " 'graduating',\n",
       " 'to',\n",
       " 'a',\n",
       " 'totally',\n",
       " 'brilliant',\n",
       " 'future',\n",
       " '.',\n",
       " 'And',\n",
       " 'here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'standing',\n",
       " 'before',\n",
       " 'you',\n",
       " 'as',\n",
       " 'president',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " '.',\n",
       " 'So',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'guessing',\n",
       " 'there',\n",
       " 'are',\n",
       " 'some',\n",
       " 'people',\n",
       " 'here',\n",
       " 'today',\n",
       " 'who',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'either',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'things',\n",
       " ',',\n",
       " 'either',\n",
       " 'one',\n",
       " ',',\n",
       " 'would',\n",
       " 'really',\n",
       " 'require',\n",
       " 'major',\n",
       " 'help',\n",
       " 'from',\n",
       " 'God',\n",
       " '.',\n",
       " 'Do',\n",
       " 'we',\n",
       " 'agree',\n",
       " '?',\n",
       " 'And',\n",
       " 'we',\n",
       " 'got',\n",
       " 'it.But',\n",
       " 'here',\n",
       " 'we',\n",
       " 'are',\n",
       " 'celebrating',\n",
       " 'together',\n",
       " 'on',\n",
       " 'this',\n",
       " 'very',\n",
       " 'joyous',\n",
       " 'occasion',\n",
       " '.',\n",
       " 'And',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'place',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'I',\n",
       " '’',\n",
       " 'd',\n",
       " 'rather',\n",
       " 'be',\n",
       " 'to',\n",
       " 'give',\n",
       " 'my',\n",
       " 'first',\n",
       " 'commencement',\n",
       " 'address',\n",
       " 'as',\n",
       " 'president',\n",
       " 'than',\n",
       " 'here',\n",
       " 'with',\n",
       " 'my',\n",
       " 'wonderful',\n",
       " 'friends',\n",
       " 'at',\n",
       " 'Liberty',\n",
       " 'University.And',\n",
       " 'I',\n",
       " 'accepted',\n",
       " 'this',\n",
       " 'invitation',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " '.',\n",
       " 'I',\n",
       " 'said',\n",
       " 'to',\n",
       " 'Jerry',\n",
       " 'that',\n",
       " 'I',\n",
       " '’',\n",
       " 'd',\n",
       " 'be',\n",
       " 'there',\n",
       " ',',\n",
       " 'and',\n",
       " 'when',\n",
       " 'I',\n",
       " 'say',\n",
       " 'something',\n",
       " 'I',\n",
       " 'mean',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are doing stemming, let see how we can do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will remove the words from the above sentences which are present in (corpus stopwords) and then do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set( stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try to do Lemmitization on our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set( stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now applying TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range (len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank much everybodi',\n",
       " 'congratul class',\n",
       " 'achiev day earn everi minut',\n",
       " 'thrill back liberti univers',\n",
       " 'third time',\n",
       " 'love set record right',\n",
       " 'alway set record',\n",
       " 'set record choic littl year sinc spoken beauti campu much chang',\n",
       " 'right class dress cap gown graduat total brilliant futur',\n",
       " 'stand presid unit state',\n",
       " 'guess peopl today thought either one thing either one would realli requir major help god',\n",
       " 'agre',\n",
       " 'got celebr togeth joyou occas',\n",
       " 'place world rather give first commenc address presid wonder friend liberti univers accept invit long time ago',\n",
       " 'said jerri say someth mean']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming is not giving any proper insights of sentences....lets try with lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range (len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank much everybody',\n",
       " 'congratulation class',\n",
       " 'achievement day earned every minute',\n",
       " 'thrilled back liberty university',\n",
       " 'third time',\n",
       " 'love setting record right',\n",
       " 'always set record',\n",
       " 'set record choice little year since spoken beautiful campus much changed',\n",
       " 'right class dressed cap gown graduating totally brilliant future',\n",
       " 'standing president united state',\n",
       " 'guessing people today thought either one thing either one would really require major help god',\n",
       " 'agree',\n",
       " 'got celebrating together joyous occasion',\n",
       " 'place world rather give first commencement address president wonderful friend liberty university accepted invitation long time ago',\n",
       " 'said jerry say something mean']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets now create Bag of Word (BOW) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x  = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 78)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 31* 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3534"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try to implement TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.4472136 0.        ... 0.        0.        0.       ]\n",
      " ...\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.2498751 0.        0.2498751 ... 0.2498751 0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
